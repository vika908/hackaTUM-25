{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d943f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn pyngrok python-multipart nest-asyncio diffusers transformers accelerate\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "import nest_asyncio\n",
    "from typing import Optional\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "from pyngrok import ngrok, conf\n",
    "import uvicorn\n",
    "import os\n",
    "import time\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "HF_TOKEN = \"\"\n",
    "NGROK_AUTH_TOKEN = \"\" # <--- PASTE YOUR NGROK TOKEN HERE\n",
    "\n",
    "# Set ngrok token\n",
    "if NGROK_AUTH_TOKEN != \"YOUR_NGROK_AUTHTOKEN_HERE\":\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "else:\n",
    "    print(\"WARNING: Ngrok auth token not set. The tunnel might expire quickly.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "print(f\"Loading Model to {device}...\")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    variant=\"fp16\" if device == \"cuda\" else None,\n",
    "    token=HF_TOKEN,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "print(\"Model Loaded.\")\n",
    "\n",
    "\n",
    "def apply_watermark_logic(image: Image.Image, target_radius=40, strength=2.5):\n",
    "    \"\"\"Applies watermark and returns the PIL Image.\"\"\"\n",
    "    img_ycbcr = image.convert(\"YCbCr\")\n",
    "    y, cb, cr = img_ycbcr.split()\n",
    "\n",
    "    y_arr = np.array(y)\n",
    "    f = np.fft.fft2(y_arr)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "\n",
    "    rows, cols = y_arr.shape\n",
    "    crow, ccol = rows // 2, cols // 2\n",
    "    y_grid, x_grid = np.ogrid[:rows, :cols]\n",
    "    mask_area = (x_grid - ccol)**2 + (y_grid - crow)**2\n",
    "\n",
    "    r_min, r_max = target_radius - 1, target_radius + 1\n",
    "    mask = (mask_area >= r_min**2) & (mask_area <= r_max**2)\n",
    "\n",
    "    fshift[mask] *= strength\n",
    "\n",
    "    f_ishift = np.fft.ifftshift(fshift)\n",
    "    img_back = np.fft.ifft2(f_ishift)\n",
    "    img_back = np.abs(img_back)\n",
    "\n",
    "    img_back = np.clip(img_back, 0, 255).astype(np.uint8)\n",
    "    watermarked_y = Image.fromarray(img_back)\n",
    "\n",
    "    final_image = Image.merge(\"YCbCr\", (watermarked_y, cb, cr))\n",
    "    return final_image.convert(\"RGB\")\n",
    "\n",
    "def get_radial_profile(fft_shift, center, max_r):\n",
    "    y, x = np.indices(fft_shift.shape)\n",
    "    r = np.sqrt((x - center[1])**2 + (y - center[0])**2)\n",
    "    r = r.astype(int)\n",
    "\n",
    "    # Handle division by zero or empty bins safely\n",
    "    tbin = np.bincount(r.ravel(), fft_shift.ravel())\n",
    "    nr = np.bincount(r.ravel())\n",
    "\n",
    "    # Avoid division by zero\n",
    "    radialprofile = np.zeros_like(tbin, dtype=float)\n",
    "    nonzero = nr > 0\n",
    "    radialprofile[nonzero] = tbin[nonzero] / nr[nonzero]\n",
    "\n",
    "    return radialprofile[:max_r]\n",
    "\n",
    "def detect_logic(image: Image.Image, target_radius=40):\n",
    "    \"\"\"Returns z-score and detection boolean.\"\"\"\n",
    "    img_gray = image.convert(\"L\")\n",
    "    img_arr = np.array(img_gray)\n",
    "\n",
    "    f = np.fft.fft2(img_arr)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude = np.abs(fshift)\n",
    "\n",
    "    center = (magnitude.shape[0] // 2, magnitude.shape[1] // 2)\n",
    "    profile = get_radial_profile(magnitude, center, max_r=target_radius + 20)\n",
    "\n",
    "    signal = profile[target_radius]\n",
    "\n",
    "    neighbors = np.concatenate([\n",
    "        profile[target_radius-5 : target_radius-2],\n",
    "        profile[target_radius+2 : target_radius+5]\n",
    "    ])\n",
    "    noise_mean = np.mean(neighbors)\n",
    "    noise_std = np.std(neighbors)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    z_score = (signal - noise_mean) / (noise_std + 1e-8)\n",
    "\n",
    "    return float(z_score)\n",
    "\n",
    "def image_to_base64(image: Image.Image) -> str:\n",
    "    \"\"\"Converts PIL Image to base64 string.\"\"\"\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "app = FastAPI(title=\"Diffusion Watermarker API\")\n",
    "\n",
    "# --- Pydantic Models ---\n",
    "class PromptRequest(BaseModel):\n",
    "    prompt: str\n",
    "    strength: float = 2.5\n",
    "    target_radius: int = 40\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    clean_image_base64: str\n",
    "    watermarked_image_base64: str\n",
    "    watermark_strength: float\n",
    "    target_radius: int\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    gpu_available: bool\n",
    "    gpu_name: Optional[str]\n",
    "\n",
    "class DetectionResponse(BaseModel):\n",
    "    z_score: float\n",
    "    is_generated: bool\n",
    "    confidence: str\n",
    "\n",
    "# --- Endpoints ---\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "def health_check():\n",
    "    \"\"\"Checks server health and GPU status.\"\"\"\n",
    "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else None\n",
    "    return {\n",
    "        \"status\": \"online\",\n",
    "        \"gpu_available\": torch.cuda.is_available(),\n",
    "        \"gpu_name\": gpu_name\n",
    "    }\n",
    "\n",
    "@app.post(\"/prompt\", response_model=GenerationResponse)\n",
    "def generate_image(request: PromptRequest):\n",
    "    \"\"\"Generates a clean image, applies watermark, returns both.\"\"\"\n",
    "    try:\n",
    "        print(f\"Generating: {request.prompt}\")\n",
    "        neg_prompt = \"blurry, low quality, deformed, ugly, bad anatomy, jpeg artifacts\"\n",
    "\n",
    "        clean_img = pipe(\n",
    "            request.prompt,\n",
    "            negative_prompt=neg_prompt,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=7.5\n",
    "        ).images[0]\n",
    "\n",
    "        wm_img = apply_watermark_logic(\n",
    "            clean_img,\n",
    "            target_radius=request.target_radius,\n",
    "            strength=request.strength\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"clean_image_base64\": image_to_base64(clean_img),\n",
    "            \"watermarked_image_base64\": image_to_base64(wm_img),\n",
    "            \"watermark_strength\": request.strength,\n",
    "            \"target_radius\": request.target_radius\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/detect\", response_model=DetectionResponse)\n",
    "async def detect_image(file: UploadFile = File(...)):\n",
    "    \"\"\"Upload an image to detect if it was generated/watermarked.\"\"\"\n",
    "    try:\n",
    "        contents = await file.read()\n",
    "        image = Image.open(io.BytesIO(contents))\n",
    "\n",
    "        # Run detection (default radius 40 based on your notebook)\n",
    "        z_score = detect_logic(image, target_radius=40)\n",
    "\n",
    "        # Determine Threshold (Using 3.0 as standard deviation threshold)\n",
    "        is_detected = z_score > 3.0\n",
    "\n",
    "        # Simple confidence logic\n",
    "        if z_score > 6.0: confidence = \"Very High\"\n",
    "        elif z_score > 3.0: confidence = \"High\"\n",
    "        elif z_score > 2.0: confidence = \"Suspect\"\n",
    "        else: confidence = \"Low\"\n",
    "\n",
    "        return {\n",
    "            \"z_score\": z_score,\n",
    "            \"is_generated\": is_detected,\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "print(\"Cleaning up old tunnels...\")\n",
    "ngrok.kill()\n",
    "os.system(\"pkill ngrok\")\n",
    "time.sleep(2) # Give it a moment to release the port\n",
    "\n",
    "try:\n",
    "    public_url = ngrok.connect(8000).public_url\n",
    "    print(f\"\\n\ud83d\ude80 SERVER IS LIVE! Access Swagger UI here: {public_url}/docs\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Ngrok connection failed: {e}\")\n",
    "    print(\"Trying again with default config...\")\n",
    "    # Fallback in case of config issues\n",
    "    conf.get_default().region = \"us\"\n",
    "    public_url = ngrok.connect(8000).public_url\n",
    "    print(f\"\\n\ud83d\ude80 SERVER IS LIVE! Access Swagger UI here: {public_url}/docs\\n\")\n",
    "\n",
    "config = uvicorn.Config(app, port=8000, log_level=\"info\")\n",
    "server = uvicorn.Server(config)\n",
    "await server.serve()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}