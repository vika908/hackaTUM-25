{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79cb5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers fastapi nest-asyncio pyngrok uvicorn\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import threading\n",
    "from typing import List, Optional\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList\n",
    "\n",
    "CONFIG = {\n",
    "    \"keys\": [\n",
    "        654, 123, 876, 345, 987, 234, 765, 432,\n",
    "        111, 222, 333, 444, 555, 666, 777, 888, 999, 1010, 2020, 3030\n",
    "    ],\n",
    "    \"ngram_len\": 5,\n",
    "    \"watermark_bias\": 2.0\n",
    "}\n",
    "\n",
    "def inject_invisible_watermark(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Injects a zero-width space (\\u200b) after every 3rd word.\n",
    "    This is invisible to the user but detectable by code.\n",
    "    \"\"\"\n",
    "    words = text.split(\" \")\n",
    "    # We create a new list to avoid modifying while iterating logic issues\n",
    "    new_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        new_words.append(word)\n",
    "        # Add marker every 3 words (index 2, 5, 8...)\n",
    "        if (i + 1) % 3 == 0:\n",
    "            new_words[-1] = new_words[-1] + \"\\u200b\"\n",
    "\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def compute_g_values_for_token(ngram_ids: List[int], next_token_id: int, key: int) -> float:\n",
    "    data = f\"{ngram_ids}-{next_token_id}-{key}\".encode('utf-8')\n",
    "    hash_bytes = hashlib.sha256(data).digest()\n",
    "    hash_int = int.from_bytes(hash_bytes[:4], 'big')\n",
    "    return hash_int / (2**32 - 1)\n",
    "\n",
    "def numpy_weighted_mean_score(g_values: np.ndarray, mask: np.ndarray, weights: Optional[np.ndarray] = None) -> float:\n",
    "    watermarking_depth = g_values.shape[-1]\n",
    "    if weights is None: weights = np.ones(watermarking_depth)\n",
    "    weights = weights * (watermarking_depth / np.sum(weights))\n",
    "    weighted_g = g_values * weights\n",
    "    score_per_token = np.sum(weighted_g, axis=1) / watermarking_depth\n",
    "    num_unmasked = np.sum(mask)\n",
    "    if num_unmasked == 0: return 0.0\n",
    "    total_score = np.sum(score_per_token * mask)\n",
    "    return float(total_score / num_unmasked)\n",
    "\n",
    "class CustomDetector:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ngram_len = CONFIG[\"ngram_len\"]\n",
    "        self.keys = CONFIG[\"keys\"]\n",
    "\n",
    "    def score(self, text: str) -> float:\n",
    "        # Standard Statistical Scoring\n",
    "        ids = self.tokenizer(text, add_special_tokens=True)['input_ids']\n",
    "        if len(ids) <= self.ngram_len: return 0.0\n",
    "        seq_len = len(ids) - self.ngram_len\n",
    "\n",
    "        g_vals = []\n",
    "        mask = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            current_token_idx = i + self.ngram_len\n",
    "            context_ngram = ids[i : i+self.ngram_len]\n",
    "            token = ids[current_token_idx]\n",
    "\n",
    "            row_g_vals = []\n",
    "            for key in self.keys:\n",
    "                row_g_vals.append(compute_g_values_for_token(context_ngram, token, key))\n",
    "            g_vals.append(row_g_vals)\n",
    "            mask.append(1.0)\n",
    "\n",
    "        if not g_vals: return 0.0\n",
    "        return numpy_weighted_mean_score(np.array(g_vals), np.array(mask))\n",
    "\n",
    "class PurePythonWatermarkLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, ngram_len, keys, bias):\n",
    "        self.ngram_len = ngram_len\n",
    "        self.keys = keys\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        current_ids = input_ids[0].tolist()\n",
    "        if len(current_ids) < self.ngram_len: return scores\n",
    "        ngram = current_ids[-self.ngram_len:]\n",
    "\n",
    "        # Optimization: Only check top 50 tokens to save time\n",
    "        top_values, top_indices = torch.topk(scores, 50, dim=1)\n",
    "\n",
    "        for idx in top_indices[0]:\n",
    "            token_id = idx.item()\n",
    "            g_sum = sum(compute_g_values_for_token(ngram, token_id, key) for key in self.keys)\n",
    "            avg_g = g_sum / len(self.keys)\n",
    "            if avg_g > 0.5:\n",
    "                scores[0, token_id] += (avg_g * self.bias)\n",
    "        return scores\n",
    "\n",
    "print(\"\u23f3 Loading Model (Gemma-2B)...\")\n",
    "HF_TOKEN = \"\" # <--- REPLACE\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(\"\u2699\ufe0f  Initializing Detector Engine...\")\n",
    "detector_engine = CustomDetector(tokenizer)\n",
    "\n",
    "app = FastAPI()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class Req(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/prompt\")\n",
    "async def generate(req: Req):\n",
    "    inputs = tokenizer(req.text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    clean_out = model.generate(**inputs, max_new_tokens=80, do_sample=True)\n",
    "    clean_txt = tokenizer.decode(clean_out[0], skip_special_tokens=True)\n",
    "\n",
    "    wm_processor = PurePythonWatermarkLogitsProcessor(\n",
    "        ngram_len=CONFIG[\"ngram_len\"],\n",
    "        keys=CONFIG[\"keys\"],\n",
    "        bias=CONFIG[\"watermark_bias\"]\n",
    "    )\n",
    "    wm_out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        do_sample=True,\n",
    "        logits_processor=LogitsProcessorList([wm_processor])\n",
    "    )\n",
    "    raw_wm_txt = tokenizer.decode(wm_out[0], skip_special_tokens=True)\n",
    "\n",
    "    final_wm_txt = inject_invisible_watermark(raw_wm_txt)\n",
    "\n",
    "    return {\"clean\": clean_txt, \"watermarked\": final_wm_txt}\n",
    "\n",
    "@app.post(\"/detect\")\n",
    "async def detect(req: Req):\n",
    "    text = req.text\n",
    "\n",
    "    # CHECK 1: Invisible Symbol (Hard Match)\n",
    "    if \"\\u200b\" in text:\n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"verdict\": \"Watermarked (Hidden Tag Found)\",\n",
    "            \"method\": \"Deterministic\"\n",
    "        }\n",
    "\n",
    "    # CHECK 2: Statistical (Fallback)\n",
    "    score = detector_engine.score(text)\n",
    "\n",
    "    return {\n",
    "        \"score\": score,\n",
    "        \"verdict\": \"Watermarked\" if score > 0.60 else \"Clean\",\n",
    "        \"method\": \"Statistical\"\n",
    "    }\n",
    "\n",
    "NGROK_TOKEN = \"\" # <--- REPLACE\n",
    "\n",
    "if NGROK_TOKEN:\n",
    "    ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "import os\n",
    "os.system(\"fuser -k 8000/tcp\")\n",
    "ngrok.kill()\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "thread = threading.Thread(target=run_server)\n",
    "thread.start()\n",
    "\n",
    "try:\n",
    "    public_url = ngrok.connect(8000).public_url\n",
    "    print(f\"\\n\ud83d\ude80 API IS LIVE AT: {public_url}\")\n",
    "    print(f\"\ud83d\udcc4 DOCS: {public_url}/docs\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting ngrok: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}